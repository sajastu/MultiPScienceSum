{"paper_id": "SP:7f160388affa492f33c54bd9dc7254765c2993d2", "sections_txt_tokenized": [{"text": "Abstractive multi document summarization has evolved as a task through the basic sequence to sequence approaches to transformer and graph based techniques. Each of these approaches has primarily focused on the issues of multi document information synthesis and attention based approaches to extract salient information. A challenge that arises with multi document summarization which is not prevalent in single document summarization is the need to effectively summarize multiple documents that might have conflicting polarity, sentiment or information about a given topic. In this paper we propose ACM, attribute conditioned multi document summarization, a model that incorporates an attribute conditioning module in order to decouple conflicting information by conditioning for a certain attribute in the output summary. This approach shows strong gains in ROGUE score over baseline multi document summarization approaches and shows gains in fluency and informativeness as shown through a human annotation analysis study.ive multi document summarization has evolved as a task through the basic sequence to sequence approaches to transformer and graph based techniques. Each of these approaches has primarily focused on the issues of multi document information synthesis and attention based approaches to extract salient information. A challenge that arises with multi document summarization which is not prevalent in single document summarization is the need to effectively summarize multiple documents that might have conflicting polarity, sentiment or information about a given topic. In this paper we propose ACM, attribute conditioned multi document summarization, a model that incorporates an attribute conditioning module in order to decouple conflicting information by conditioning for a certain attribute in the output summary. This approach shows strong gains in ROGUE score over baseline multi document summarization approaches and shows gains in fluency and informativeness as shown through a human annotation analysis study.", "tokenized": [["Abstractive", "multi", "document", "summarization", "has", "evolved", "as", "a", "task", "through", "the", "basic", "sequence", "to", "sequence", "approaches", "to", "transformer", "and", "graph", "based", "techniques", "."], ["Each", "of", "these", "approaches", "has", "primarily", "focused", "on", "the", "issues", "of", "multi", "document", "information", "synthesis", "and", "attention", "based", "approaches", "to", "extract", "salient", "information", "."], ["A", "challenge", "that", "arises", "with", "multi", "document", "summarization", "which", "is", "not", "prevalent", "in", "single", "document", "summarization", "is", "the", "need", "to", "effectively", "summarize", "multiple", "documents", "that", "might", "have", "conflicting", "polarity", ",", "sentiment", "or", "information", "about", "a", "given", "topic", "."], ["In", "this", "paper", "we", "propose", "ACM", ",", "attribute", "conditioned", "multi", "document", "summarization", ",", "a", "model", "that", "incorporates", "an", "attribute", "conditioning", "module", "in", "order", "to", "decouple", "conflicting", "information", "by", "conditioning", "for", "a", "certain", "attribute", "in", "the", "output", "summary", "."], ["This", "approach", "shows", "strong", "gains", "in", "ROGUE", "score", "over", "baseline", "multi", "document", "summarization", "approaches", "and", "shows", "gains", "in", "fluency", "and", "informativeness", "as", "shown", "through", "a", "human", "annotation", "analysis", "study.ive", "multi", "document", "summarization", "has", "evolved", "as", "a", "task", "through", "the", "basic", "sequence", "to", "sequence", "approaches", "to", "transformer", "and", "graph", "based", "techniques", "."], ["Each", "of", "these", "approaches", "has", "primarily", "focused", "on", "the", "issues", "of", "multi", "document", "information", "synthesis", "and", "attention", "based", "approaches", "to", "extract", "salient", "information", "."], ["A", "challenge", "that", "arises", "with", "multi", "document", "summarization", "which", "is", "not", "prevalent", "in", "single", "document", "summarization", "is", "the", "need", "to", "effectively", "summarize", "multiple", "documents", "that", "might", "have", "conflicting", "polarity", ",", "sentiment", "or", "information", "about", "a", "given", "topic", "."], ["In", "this", "paper", "we", "propose", "ACM", ",", "attribute", "conditioned", "multi", "document", "summarization", ",", "a", "model", "that", "incorporates", "an", "attribute", "conditioning", "module", "in", "order", "to", "decouple", "conflicting", "information", "by", "conditioning", "for", "a", "certain", "attribute", "in", "the", "output", "summary", "."], ["This", "approach", "shows", "strong", "gains", "in", "ROGUE", "score", "over", "baseline", "multi", "document", "summarization", "approaches", "and", "shows", "gains", "in", "fluency", "and", "informativeness", "as", "shown", "through", "a", "human", "annotation", "analysis", "study", "."]]}, {"text": "Abstractive multi document summarization is the task of writing a single summary of the key points and content in multiple related documents. This task has evolved from research in single document abstractive and extractive summarization however it faces unique challenges due to the duplicate content, conflicting content, a larger body of text as well as inter document connections between ideas. ((?) This task has evolved from early approaches using sequence to sequence (Seq2Seq) neural architectures to transformer based architectures and the introduction of large-scale datasets ((?) (?)). Beyond the introduction of approaches now commonly used for single document abstractive summarization, cross document attention and graphs that capture relations between text in various documents have further improved the state of the art for multi document summarization tasks. ((?), (?)). These graphs aim to better represent the inter dependencies between articles by representing text spans as nodes in the graph and capturing the relations between these sentences as edge weights.\nDespite the advances made with these approaches, a significant challenge remains with multi document summarization with respect to how to deal with contradictory information present in the multiple source documents. It is critical to both learn the relationships between different documents as well as to extract salient information that is consistent with the output viewpoint. This is a situation often faced with summarizing multiple news articles where different viewpoints on an issue can significantly change the semantic structure of the content present in each article making it challenging for the abstractive summarization model to learn the relationships between inconsistent or conflicting information.\nThis paper proposes ACM, attribute conditioned multi document summarization, a novel approach that incorporates an attribute conditioning module with abstractive multi document summarization in order to condition for a particular attribute when generating the multi document summary. This approach addresses the challenge of dealing with conflicting information in the input documents by conditioning for a particular attribute in the input text. We further analyze the contributions of conditioning with this attribute model by using a weighting term to condition for the attribute when learning a graphical representation of the input documents, during train time and during evaluation. The attribute conditioning model is trained in order to determine view point consistency through sentiment and polarity classification datasets, however this module is highly composable and can be conditioned for other attributes as well. We train these classification models on every input prefix\nin the input dataset in order to train the model to be agnostic to input text length. We evaluate these approaches on baseline abstractive multi document summarization architectures in order to observe improvements in the output consistency evaluated through ROGUE metrics and through human annotations for fluency, informativeness and consistency. Our approach consists of individual composable elements, each of which we further evaluate independently through ablation studies.\n\u2022 We learn a graphical representation of the input documents that weights graph edges incorporating both the conditional attribute score for each input as well as the cosine similarity.\n\u2022 We conditionally fine tune the abstractive multi document summarization module by combining the logits for each input prefix passed to the decoder module with the conditioning score for that prefix from the attribute conditioning module.\n\u2022 When evaluating the model, we modify beam search to rank each beam according to the product of the attribute conditioning model and the conditional likelihood score.\nThe contributions of our work are as follows:\n\u2022 We train an attribute conditioning MDS model to address conflicting information in input documents and show that our architecture combining this module with abstractive multi document summarization improves ROGUE metrics for the output summary.\n\u2022 We provide a human annotation analysis on the output summaries evaluating for informativeness, consistency and fluency.\n\u2022 We analyze the contributions of each composable element in our model.", "tokenized": [["Abstractive", "multi", "document", "summarization", "is", "the", "task", "of", "writing", "a", "single", "summary", "of", "the", "key", "points", "and", "content", "in", "multiple", "related", "documents", "."], ["This", "task", "has", "evolved", "from", "research", "in", "single", "document", "abstractive", "and", "extractive", "summarization", "however", "it", "faces", "unique", "challenges", "due", "to", "the", "duplicate", "content", ",", "conflicting", "content", ",", "a", "larger", "body", "of", "text", "as", "well", "as", "inter", "document", "connections", "between", "ideas", ".", "(", "(", "?", ")", "This", "task", "has", "evolved", "from", "early", "approaches", "using", "sequence", "to", "sequence", "(", "Seq2Seq", ")", "neural", "architectures", "to", "transformer", "based", "architectures", "and", "the", "introduction", "of", "large-scale", "datasets", "(", "(", "?", ")", "(", "?", ")", ")", "."], ["Beyond", "the", "introduction", "of", "approaches", "now", "commonly", "used", "for", "single", "document", "abstractive", "summarization", ",", "cross", "document", "attention", "and", "graphs", "that", "capture", "relations", "between", "text", "in", "various", "documents", "have", "further", "improved", "the", "state", "of", "the", "art", "for", "multi", "document", "summarization", "tasks", "."], ["(", "(", "?", ")", ",", "(", "?", ")", ")", "."], ["These", "graphs", "aim", "to", "better", "represent", "the", "inter", "dependencies", "between", "articles", "by", "representing", "text", "spans", "as", "nodes", "in", "the", "graph", "and", "capturing", "the", "relations", "between", "these", "sentences", "as", "edge", "weights", "."], ["\n", "Despite", "the", "advances", "made", "with", "these", "approaches", ",", "a", "significant", "challenge", "remains", "with", "multi", "document", "summarization", "with", "respect", "to", "how", "to", "deal", "with", "contradictory", "information", "present", "in", "the", "multiple", "source", "documents", "."], ["It", "is", "critical", "to", "both", "learn", "the", "relationships", "between", "different", "documents", "as", "well", "as", "to", "extract", "salient", "information", "that", "is", "consistent", "with", "the", "output", "viewpoint", "."], ["This", "is", "a", "situation", "often", "faced", "with", "summarizing", "multiple", "news", "articles", "where", "different", "viewpoints", "on", "an", "issue", "can", "significantly", "change", "the", "semantic", "structure", "of", "the", "content", "present", "in", "each", "article", "making", "it", "challenging", "for", "the", "abstractive", "summarization", "model", "to", "learn", "the", "relationships", "between", "inconsistent", "or", "conflicting", "information", "."], ["\n", "This", "paper", "proposes", "ACM", ",", "attribute", "conditioned", "multi", "document", "summarization", ",", "a", "novel", "approach", "that", "incorporates", "an", "attribute", "conditioning", "module", "with", "abstractive", "multi", "document", "summarization", "in", "order", "to", "condition", "for", "a", "particular", "attribute", "when", "generating", "the", "multi", "document", "summary", "."], ["This", "approach", "addresses", "the", "challenge", "of", "dealing", "with", "conflicting", "information", "in", "the", "input", "documents", "by", "conditioning", "for", "a", "particular", "attribute", "in", "the", "input", "text", "."], ["We", "further", "analyze", "the", "contributions", "of", "conditioning", "with", "this", "attribute", "model", "by", "using", "a", "weighting", "term", "to", "condition", "for", "the", "attribute", "when", "learning", "a", "graphical", "representation", "of", "the", "input", "documents", ",", "during", "train", "time", "and", "during", "evaluation", "."], ["The", "attribute", "conditioning", "model", "is", "trained", "in", "order", "to", "determine", "view", "point", "consistency", "through", "sentiment", "and", "polarity", "classification", "datasets", ",", "however", "this", "module", "is", "highly", "composable", "and", "can", "be", "conditioned", "for", "other", "attributes", "as", "well", "."], ["We", "train", "these", "classification", "models", "on", "every", "input", "prefix", "\n", "in", "the", "input", "dataset", "in", "order", "to", "train", "the", "model", "to", "be", "agnostic", "to", "input", "text", "length", "."], ["We", "evaluate", "these", "approaches", "on", "baseline", "abstractive", "multi", "document", "summarization", "architectures", "in", "order", "to", "observe", "improvements", "in", "the", "output", "consistency", "evaluated", "through", "ROGUE", "metrics", "and", "through", "human", "annotations", "for", "fluency", ",", "informativeness", "and", "consistency", "."], ["Our", "approach", "consists", "of", "individual", "composable", "elements", ",", "each", "of", "which", "we", "further", "evaluate", "independently", "through", "ablation", "studies", "."], ["\n", "\u2022", "We", "learn", "a", "graphical", "representation", "of", "the", "input", "documents", "that", "weights", "graph", "edges", "incorporating", "both", "the", "conditional", "attribute", "score", "for", "each", "input", "as", "well", "as", "the", "cosine", "similarity", "."], ["\n", "\u2022", "We", "conditionally", "fine", "tune", "the", "abstractive", "multi", "document", "summarization", "module", "by", "combining", "the", "logits", "for", "each", "input", "prefix", "passed", "to", "the", "decoder", "module", "with", "the", "conditioning", "score", "for", "that", "prefix", "from", "the", "attribute", "conditioning", "module", "."], ["\n", "\u2022", "When", "evaluating", "the", "model", ",", "we", "modify", "beam", "search", "to", "rank", "each", "beam", "according", "to", "the", "product", "of", "the", "attribute", "conditioning", "model", "and", "the", "conditional", "likelihood", "score", "."], ["\n", "The", "contributions", "of", "our", "work", "are", "as", "follows", ":", "\n", "\u2022", "We", "train", "an", "attribute", "conditioning", "MDS", "model", "to", "address", "conflicting", "information", "in", "input", "documents", "and", "show", "that", "our", "architecture", "combining", "this", "module", "with", "abstractive", "multi", "document", "summarization", "improves", "ROGUE", "metrics", "for", "the", "output", "summary", "."], ["\n", "\u2022", "We", "provide", "a", "human", "annotation", "analysis", "on", "the", "output", "summaries", "evaluating", "for", "informativeness", ",", "consistency", "and", "fluency", "."], ["\n", "\u2022", "We", "analyze", "the", "contributions", "of", "each", "composable", "element", "in", "our", "model", "."]]}, {"text": "This paper develops on techniques developed through the following NLP tasks - abstractive summarization, multi document summarization, and conditional language modeling - to efficiently address the issue of decoupling conflicting attributes in multi document summarization. We define the task of incorporating conflicting attributes e.g. sentiment and polarity as defined by a classification model in abstractive summaries as an application of conditional language modeling techniques to condition for a particular attribute when generating the output summary thus removing conflicting information. Conditional language modeling is a key factor in this problem as we aim to decouple the conflicting information in the summaries by conditionally selecting which text output to work with.", "tokenized": [["This", "paper", "develops", "on", "techniques", "developed", "through", "the", "following", "NLP", "tasks", "-", "abstractive", "summarization", ",", "multi", "document", "summarization", ",", "and", "conditional", "language", "modeling", "-", "to", "efficiently", "address", "the", "issue", "of", "decoupling", "conflicting", "attributes", "in", "multi", "document", "summarization", "."], ["We", "define", "the", "task", "of", "incorporating", "conflicting", "attributes", "e.g.", "sentiment", "and", "polarity", "as", "defined", "by", "a", "classification", "model", "in", "abstractive", "summaries", "as", "an", "application", "of", "conditional", "language", "modeling", "techniques", "to", "condition", "for", "a", "particular", "attribute", "when", "generating", "the", "output", "summary", "thus", "removing", "conflicting", "information", "."], ["Conditional", "language", "modeling", "is", "a", "key", "factor", "in", "this", "problem", "as", "we", "aim", "to", "decouple", "the", "conflicting", "information", "in", "the", "summaries", "by", "conditionally", "selecting", "which", "text", "output", "to", "work", "with", "."]]}, {"text": "Conditional language modeling has been approached both through applying global constraints on text generation, by applying control only at inference time and by directly optimizing through policy gradient methods. (?), (?), (?). Alternative approaches include relying on predefined sets of control tokens or control codes as a form of a copy mechanism (?). These approaches have been successful at steering language models towards specific features or as in (?) with conditioning for the outputs to include words conditioned for through a simple bag of words attribute model. We build on these approaches to design the attribute conditioning module.", "tokenized": [["Conditional", "language", "modeling", "has", "been", "approached", "both", "through", "applying", "global", "constraints", "on", "text", "generation", ",", "by", "applying", "control", "only", "at", "inference", "time", "and", "by", "directly", "optimizing", "through", "policy", "gradient", "methods", "."], ["(", "?", ")", ",", "(", "?", ")", ",", "(", "?", ")", "."], ["Alternative", "approaches", "include", "relying", "on", "predefined", "sets", "of", "control", "tokens", "or", "control", "codes", "as", "a", "form", "of", "a", "copy", "mechanism", "(", "?", ")", "."], ["These", "approaches", "have", "been", "successful", "at", "steering", "language", "models", "towards", "specific", "features", "or", "as", "in", "(", "?", ")", "with", "conditioning", "for", "the", "outputs", "to", "include", "words", "conditioned", "for", "through", "a", "simple", "bag", "of", "words", "attribute", "model", "."], ["We", "build", "on", "these", "approaches", "to", "design", "the", "attribute", "conditioning", "module", "."]]}, {"text": "Abstractive summarization (AS) is the process by which a language model is trained to best write original text that matches a pre-generated summary for a given article. AS has gone through several phases of which the pioneering work was carried out by (?) where an Seq2Seq RNN Model was implemented to generate text. The Seq2Seq RNN Model inherently had multiple challenges such as altering factual details and redundancy. (?) circumvented the issues by creating a Pointer-Generator Model which keeps track of words and sequence in the original text and using them in the result hence ensuring the meaning of summary is in-line with the original text. This paper also included a coverage mechanism to keep of track of which parts of the original text have been summarized thus penalizing repetition. This work was built on through the development of BART, the bi-directional auto-regressive transformer. (?). BART improves on the task of abstractive summarization by introducing arbitrary noise in the input text and training the model to reconstruct the original text.", "tokenized": [["Abstractive", "summarization", "(", "AS", ")", "is", "the", "process", "by", "which", "a", "language", "model", "is", "trained", "to", "best", "write", "original", "text", "that", "matches", "a", "pre-generated", "summary", "for", "a", "given", "article", "."], ["AS", "has", "gone", "through", "several", "phases", "of", "which", "the", "pioneering", "work", "was", "carried", "out", "by", "(", "?", ")", "where", "an", "Seq2Seq", "RNN", "Model", "was", "implemented", "to", "generate", "text", "."], ["The", "Seq2Seq", "RNN", "Model", "inherently", "had", "multiple", "challenges", "such", "as", "altering", "factual", "details", "and", "redundancy", "."], ["(", "?", ")", "circumvented", "the", "issues", "by", "creating", "a", "Pointer-Generator", "Model", "which", "keeps", "track", "of", "words", "and", "sequence", "in", "the", "original", "text", "and", "using", "them", "in", "the", "result", "hence", "ensuring", "the", "meaning", "of", "summary", "is", "in-line", "with", "the", "original", "text", "."], ["This", "paper", "also", "included", "a", "coverage", "mechanism", "to", "keep", "of", "track", "of", "which", "parts", "of", "the", "original", "text", "have", "been", "summarized", "thus", "penalizing", "repetition", "."], ["This", "work", "was", "built", "on", "through", "the", "development", "of", "BART", ",", "the", "bi-directional", "auto-regressive", "transformer", ".", "(", "?", ")", "."], ["BART", "improves", "on", "the", "task", "of", "abstractive", "summarization", "by", "introducing", "arbitrary", "noise", "in", "the", "input", "text", "and", "training", "the", "model", "to", "reconstruct", "the", "original", "text", "."]]}, {"text": "Multi document summarization has evolved through four phases of primary approaches since the task was first introduced. The first set of approaches focused on graph ranking based extractive methods through TextRank (?), LexRank (?)and others. These approaches came before syntax and structure based compressive methods which aimed to tackle issues of information redundancy and paraphrasing between multiple documents. Compression-based methods as shown in (?) and paraphrasing based (?) (Bing et al 2015) were improved upon with the advent of neural seq2seq based abstractive methods in 2017. This allowed multi document summarization to further improve upon the work done with single document abstractive summarization through approaches such as pointer generator- maximal marignal relevance (?), T-DMCA (?) the paper that also introduced the foundational WikiSum dataset and HierMMR (?) that introduced MultiNews. These approaches aimed to tackle information compression through maximal marginal relevance scores across documents and through attention based mechanisms.\nImprovements since those baseline models include further leveraging graph based approaches to pre-synthesize dependencies between the articles prior to the multi document summarization as tackled in (?). Further work needs to be done to further exploit these graphical representations as (?) essentially works to establish baselines with tf-idf, cosine similarity and a graphical representation first described in (?). These papers primarily aim to address de-duplicating information and learning relationships between the different topics shared across documents. The first paper to work with addressing conflicting information across the multiple documents is (?) in their work to incorporate an opinion polarity module to the pointer generator network architecture used earlier on in multi document summarization research.", "tokenized": [["Multi", "document", "summarization", "has", "evolved", "through", "four", "phases", "of", "primary", "approaches", "since", "the", "task", "was", "first", "introduced", "."], ["The", "first", "set", "of", "approaches", "focused", "on", "graph", "ranking", "based", "extractive", "methods", "through", "TextRank", "(", "?", ")", ",", "LexRank", "(?)and", "others", "."], ["These", "approaches", "came", "before", "syntax", "and", "structure", "based", "compressive", "methods", "which", "aimed", "to", "tackle", "issues", "of", "information", "redundancy", "and", "paraphrasing", "between", "multiple", "documents", "."], ["Compression-based", "methods", "as", "shown", "in", "(", "?", ")", "and", "paraphrasing", "based", "(", "?", ")", "(", "Bing", "et", "al", "2015", ")", "were", "improved", "upon", "with", "the", "advent", "of", "neural", "seq2seq", "based", "abstractive", "methods", "in", "2017", "."], ["This", "allowed", "multi", "document", "summarization", "to", "further", "improve", "upon", "the", "work", "done", "with", "single", "document", "abstractive", "summarization", "through", "approaches", "such", "as", "pointer", "generator-", "maximal", "marignal", "relevance", "(", "?", ")", ",", "T-DMCA", "(", "?", ")", "the", "paper", "that", "also", "introduced", "the", "foundational", "WikiSum", "dataset", "and", "HierMMR", "(", "?", ")", "that", "introduced", "MultiNews", "."], ["These", "approaches", "aimed", "to", "tackle", "information", "compression", "through", "maximal", "marginal", "relevance", "scores", "across", "documents", "and", "through", "attention", "based", "mechanisms", "."], ["\n", "Improvements", "since", "those", "baseline", "models", "include", "further", "leveraging", "graph", "based", "approaches", "to", "pre-synthesize", "dependencies", "between", "the", "articles", "prior", "to", "the", "multi", "document", "summarization", "as", "tackled", "in", "(", "?", ")", "."], ["Further", "work", "needs", "to", "be", "done", "to", "further", "exploit", "these", "graphical", "representations", "as", "(", "?", ")", "essentially", "works", "to", "establish", "baselines", "with", "tf-idf", ",", "cosine", "similarity", "and", "a", "graphical", "representation", "first", "described", "in", "(", "?", ")", "."], ["These", "papers", "primarily", "aim", "to", "address", "de-duplicating", "information", "and", "learning", "relationships", "between", "the", "different", "topics", "shared", "across", "documents", "."], ["The", "first", "paper", "to", "work", "with", "addressing", "conflicting", "information", "across", "the", "multiple", "documents", "is", "(", "?", ")", "in", "their", "work", "to", "incorporate", "an", "opinion", "polarity", "module", "to", "the", "pointer", "generator", "network", "architecture", "used", "earlier", "on", "in", "multi", "document", "summarization", "research", "."]]}, {"text": "We present a novel technique ACM, attribute conditioned multi document summarization, which is designed to address the problem of resolving conflicting information in multi document summarization through the use of an attribute conditioning module. Conflicting information is determined by the attribute conditioning model trained on both a polarization and a sentiment analysis dataset since both factor into determining information consistency. XLNet (?) is used as the model architecture for the attribute conditioning module and is trained using sentence prefixes in order to capture both word level and phrase or sentence level features. XLNet has shown state of the art results in sentiment analysis tasks. The outputs of this classifier are used in each approach in order to fine tune the model to consistently condition for a particular attribute.\nIn order to preserve viewpoint consistency, the attribute conditioning module is used to guide each stage of the summarization process. To prevent over fitting to the conditioning model, each stage is conditioned with a weighting term set as a hyper parameter when training ACM. Each of these approaches is explained in more detail in the following sections. We also include each approach as an ablation study in order to determine how effective each stage of conditioning is on the final output summaries. In these ablation studies, we aim to investigate how well the attribute conditioning model can show improvements over three different baseline abstractive multi document summarization models - BART (?), BART with longformer self attention (?) and graph sum (?).", "tokenized": [["We", "present", "a", "novel", "technique", "ACM", ",", "attribute", "conditioned", "multi", "document", "summarization", ",", "which", "is", "designed", "to", "address", "the", "problem", "of", "resolving", "conflicting", "information", "in", "multi", "document", "summarization", "through", "the", "use", "of", "an", "attribute", "conditioning", "module", "."], ["Conflicting", "information", "is", "determined", "by", "the", "attribute", "conditioning", "model", "trained", "on", "both", "a", "polarization", "and", "a", "sentiment", "analysis", "dataset", "since", "both", "factor", "into", "determining", "information", "consistency", "."], ["XLNet", "(", "?", ")", "is", "used", "as", "the", "model", "architecture", "for", "the", "attribute", "conditioning", "module", "and", "is", "trained", "using", "sentence", "prefixes", "in", "order", "to", "capture", "both", "word", "level", "and", "phrase", "or", "sentence", "level", "features", "."], ["XLNet", "has", "shown", "state", "of", "the", "art", "results", "in", "sentiment", "analysis", "tasks", "."], ["The", "outputs", "of", "this", "classifier", "are", "used", "in", "each", "approach", "in", "order", "to", "fine", "tune", "the", "model", "to", "consistently", "condition", "for", "a", "particular", "attribute", "."], ["\n", "In", "order", "to", "preserve", "viewpoint", "consistency", ",", "the", "attribute", "conditioning", "module", "is", "used", "to", "guide", "each", "stage", "of", "the", "summarization", "process", "."], ["To", "prevent", "over", "fitting", "to", "the", "conditioning", "model", ",", "each", "stage", "is", "conditioned", "with", "a", "weighting", "term", "set", "as", "a", "hyper", "parameter", "when", "training", "ACM", "."], ["Each", "of", "these", "approaches", "is", "explained", "in", "more", "detail", "in", "the", "following", "sections", "."], ["We", "also", "include", "each", "approach", "as", "an", "ablation", "study", "in", "order", "to", "determine", "how", "effective", "each", "stage", "of", "conditioning", "is", "on", "the", "final", "output", "summaries", "."], ["In", "these", "ablation", "studies", ",", "we", "aim", "to", "investigate", "how", "well", "the", "attribute", "conditioning", "model", "can", "show", "improvements", "over", "three", "different", "baseline", "abstractive", "multi", "document", "summarization", "models", "-", "BART", "(", "?", ")", ",", "BART", "with", "longformer", "self", "attention", "(", "?", ")", "and", "graph", "sum", "(", "?", ")", "."]]}, {"text": "We preprocess the inputs to the model through a graph conditional weighting layer. As shown in diagram 2, for each of the input paragraph vectors both the positional encoding layer and the attribute conditioning layer outputs are computed. The graph representation is then constructed by creating a matrix of values where each row and column represents a sentence in one of the input documents. The baseline model for GraphSum (?) computed a similarity score between these paragraphs by using tf-idf. In our proposal, we aim to improve this approach by also incorporating the similarity between the attribute scores between each paragraph. Thus the graph will learn stronger weights between paragraphs that are of the same polarity or sentiment and conditionally select for those when generating the output summary.\nAfter generation, this graph is passed into the transformer encoder matrix as a set of attention weights for the graph encoder layers in combination with the standard transformer encoder layers in the model. Each sentence is passed through the conditional model and the score is multiplied and normalized before generating the matrix of weights for the transformer encoder layers.", "tokenized": [["We", "preprocess", "the", "inputs", "to", "the", "model", "through", "a", "graph", "conditional", "weighting", "layer", "."], ["As", "shown", "in", "diagram", "2", ",", "for", "each", "of", "the", "input", "paragraph", "vectors", "both", "the", "positional", "encoding", "layer", "and", "the", "attribute", "conditioning", "layer", "outputs", "are", "computed", "."], ["The", "graph", "representation", "is", "then", "constructed", "by", "creating", "a", "matrix", "of", "values", "where", "each", "row", "and", "column", "represents", "a", "sentence", "in", "one", "of", "the", "input", "documents", "."], ["The", "baseline", "model", "for", "GraphSum", "(", "?", ")", "computed", "a", "similarity", "score", "between", "these", "paragraphs", "by", "using", "tf-idf", "."], ["In", "our", "proposal", ",", "we", "aim", "to", "improve", "this", "approach", "by", "also", "incorporating", "the", "similarity", "between", "the", "attribute", "scores", "between", "each", "paragraph", "."], ["Thus", "the", "graph", "will", "learn", "stronger", "weights", "between", "paragraphs", "that", "are", "of", "the", "same", "polarity", "or", "sentiment", "and", "conditionally", "select", "for", "those", "when", "generating", "the", "output", "summary", "."], ["\n", "After", "generation", ",", "this", "graph", "is", "passed", "into", "the", "transformer", "encoder", "matrix", "as", "a", "set", "of", "attention", "weights", "for", "the", "graph", "encoder", "layers", "in", "combination", "with", "the", "standard", "transformer", "encoder", "layers", "in", "the", "model", "."], ["Each", "sentence", "is", "passed", "through", "the", "conditional", "model", "and", "the", "score", "is", "multiplied", "and", "normalized", "before", "generating", "the", "matrix", "of", "weights", "for", "the", "transformer", "encoder", "layers", "."]]}, {"text": "We treat the problem through a Bayesian factorization approach effectively decoupling the pretrained summarization model from the fine tuning classification model. This approach is described as using a future discriminator as it takes the entire sequence of text generated so far, appends to it\nthe most likely next token predicted by the summarization model and then computes the likelihood that this sequence satisfies the desired classification attribute. In doing so it increases the likelihood of both generating text that satisfies the summarization model with a high likelihood as well as staying consistent with the desired text modeling attributes.\nWe can model the standard summarization task as\nP (X) = \u03a0ni P (xi|x1:i\u22121) Let X refer to a set of input documents d1, ..dn pertaining to a specific MDS summary si. Let each document di = xi1, ..xin where xij refers to token j in document i. Prior to the MDS task, all input documents are appended, thus absolving the need to index each token with its corresponding document. Lastly, let a refer to the conditioning attribute. When conditioning on a certain attribute, we model the task as P (X|a) = \u03a0ni P (x1:i\u22121, a) We rely on the following factorization to decouple the summarization module from the attribute conditioning model: P (X|a) \u221d P (a|x1:i)P (xi|x1:i\u22121) This approach allows us to train the conditional model and the summarization model separately and in a composable manner to achieve the desired output conditioning.\nResult: Conditional MDS with Future Discriminators beams = []\nbeamWidth = 200 scores = []; for t = 1...T do\ntopBeams = argmax(beams, beamWidth); for beam \u2208 topBeams do\ntopNextTokenLogits = argmax(nextTokenLogits, 200) topNextTokens = vocabToIndex(topNextTokenLogits) combinedScores = []\nfor token, tokenLogits \u2208 (topNextTokens, topNextTokenLogits) do beam\u2019 = beam + token\nconditionalLogits = conditionalModel(beam\u2019) combinedScores += conditionalLogits*topNextTokenLogits beams = beams \u22c3 beam\u2032\nend end\nend\nFigure 1: Future Discriminators for MDS Beam Search incorporates the attribute conditioning model at evaluation time in order to weight each beam based on the combined attribute score and\nmaximum log likelihood.\nAs both the summarization model and the classification models are pretrained, during evaluation, each of the top k words selected by the decoder logits in the summarization model is added to the best sequence so far and then passed into the attribute classification model. The output probabilities of both models are then combined in selecting the next best word generated by the combined model. Technically this allows for a high degree of composability as each of the attribute models can be layered on top of each other and be added to the final logits with different weights. This shows the Bayesian factorization approach used for conditionally generating text based on the selected attribute classification model.", "tokenized": [["We", "treat", "the", "problem", "through", "a", "Bayesian", "factorization", "approach", "effectively", "decoupling", "the", "pretrained", "summarization", "model", "from", "the", "fine", "tuning", "classification", "model", "."], ["This", "approach", "is", "described", "as", "using", "a", "future", "discriminator", "as", "it", "takes", "the", "entire", "sequence", "of", "text", "generated", "so", "far", ",", "appends", "to", "it", "\n", "the", "most", "likely", "next", "token", "predicted", "by", "the", "summarization", "model", "and", "then", "computes", "the", "likelihood", "that", "this", "sequence", "satisfies", "the", "desired", "classification", "attribute", "."], ["In", "doing", "so", "it", "increases", "the", "likelihood", "of", "both", "generating", "text", "that", "satisfies", "the", "summarization", "model", "with", "a", "high", "likelihood", "as", "well", "as", "staying", "consistent", "with", "the", "desired", "text", "modeling", "attributes", "."], ["\n", "We", "can", "model", "the", "standard", "summarization", "task", "as", "\n", "P", "(", "X", ")", "=", "\u03a0ni", "P", "(", "xi|x1:i\u22121", ")", "Let", "X", "refer", "to", "a", "set", "of", "input", "documents", "d1", ",", "..", "dn", "pertaining", "to", "a", "specific", "MDS", "summary", "si", ".", "Let", "each", "document", "di", "=", "xi1", ",", "..", "xin", "where", "xij", "refers", "to", "token", "j", "in", "document", "i.", "Prior", "to", "the", "MDS", "task", ",", "all", "input", "documents", "are", "appended", ",", "thus", "absolving", "the", "need", "to", "index", "each", "token", "with", "its", "corresponding", "document", "."], ["Lastly", ",", "let", "a", "refer", "to", "the", "conditioning", "attribute", "."], ["When", "conditioning", "on", "a", "certain", "attribute", ",", "we", "model", "the", "task", "as", "P", "(", "X|a", ")", "=", "\u03a0ni", "P", "(", "x1:i\u22121", ",", "a", ")", "We", "rely", "on", "the", "following", "factorization", "to", "decouple", "the", "summarization", "module", "from", "the", "attribute", "conditioning", "model", ":", "P", "(", "X|a", ")", "\u221d", "P", "(a|x1:i)P", "(", "xi|x1:i\u22121", ")", "This", "approach", "allows", "us", "to", "train", "the", "conditional", "model", "and", "the", "summarization", "model", "separately", "and", "in", "a", "composable", "manner", "to", "achieve", "the", "desired", "output", "conditioning", "."], ["\n", "Result", ":", "Conditional", "MDS", "with", "Future", "Discriminators", "beams", "=", "[", "]", "\n", "beamWidth", "=", "200", "scores", "=", "[", "]", ";", "for", "t", "=", "1", "...", "T", "do", "\n", "topBeams", "=", "argmax(beams", ",", "beamWidth", ")", ";", "for", "beam", "\u2208", "topBeams", "do", "\n", "topNextTokenLogits", "=", "argmax(nextTokenLogits", ",", "200", ")", "topNextTokens", "=", "vocabToIndex(topNextTokenLogits", ")", "combinedScores", "=", "[", "]", "\n", "for", "token", ",", "tokenLogits", "\u2208", "(", "topNextTokens", ",", "topNextTokenLogits", ")", "do", "beam", "\u2019", "=", "beam", "+", "token", "\n", "conditionalLogits", "=", "conditionalModel(beam", "\u2019", ")", "combinedScores", "+", "=", "conditionalLogits*topNextTokenLogits", "beams", "=", "beams", "\u22c3", "beam\u2032", "\n", "end", "end", "\n", "end", "\n", "Figure", "1", ":", "Future", "Discriminators", "for", "MDS", "Beam", "Search", "incorporates", "the", "attribute", "conditioning", "model", "at", "evaluation", "time", "in", "order", "to", "weight", "each", "beam", "based", "on", "the", "combined", "attribute", "score", "and", "\n", "maximum", "log", "likelihood", "."], ["\n", "As", "both", "the", "summarization", "model", "and", "the", "classification", "models", "are", "pretrained", ",", "during", "evaluation", ",", "each", "of", "the", "top", "k", "words", "selected", "by", "the", "decoder", "logits", "in", "the", "summarization", "model", "is", "added", "to", "the", "best", "sequence", "so", "far", "and", "then", "passed", "into", "the", "attribute", "classification", "model", "."], ["The", "output", "probabilities", "of", "both", "models", "are", "then", "combined", "in", "selecting", "the", "next", "best", "word", "generated", "by", "the", "combined", "model", "."], ["Technically", "this", "allows", "for", "a", "high", "degree", "of", "composability", "as", "each", "of", "the", "attribute", "models", "can", "be", "layered", "on", "top", "of", "each", "other", "and", "be", "added", "to", "the", "final", "logits", "with", "different", "weights", "."], ["This", "shows", "the", "Bayesian", "factorization", "approach", "used", "for", "conditionally", "generating", "text", "based", "on", "the", "selected", "attribute", "classification", "model", "."]]}, {"text": "At each time step of training, we combine the probability distribution over the next words generated by the decoder with the polarity scores as determined by the attribute classification model. In contrast to other models such as (?), this allows the base model to be conditioned to output based on\nthe desired attribute similar to the approaches taken to train class conditional language models. An additional step is taken to A diagram of this model can be seen in Fig. 2.\nFor each model architecture, the decoder logits are multiplied with the attribute model\u2019s logits during during training. This trains the abstractive multi document summarization model to output text conditioned on that particular attribute. The same classifier models and baseline MDS models were used to evaluate this approach as in the previous method. We show that this approach can be applied to any baseline MDS architecture in the follow up ablations.", "tokenized": [["At", "each", "time", "step", "of", "training", ",", "we", "combine", "the", "probability", "distribution", "over", "the", "next", "words", "generated", "by", "the", "decoder", "with", "the", "polarity", "scores", "as", "determined", "by", "the", "attribute", "classification", "model", "."], ["In", "contrast", "to", "other", "models", "such", "as", "(", "?", ")", ",", "this", "allows", "the", "base", "model", "to", "be", "conditioned", "to", "output", "based", "on", "\n", "the", "desired", "attribute", "similar", "to", "the", "approaches", "taken", "to", "train", "class", "conditional", "language", "models", "."], ["An", "additional", "step", "is", "taken", "to", "A", "diagram", "of", "this", "model", "can", "be", "seen", "in", "Fig.", "2", "."], ["\n", "For", "each", "model", "architecture", ",", "the", "decoder", "logits", "are", "multiplied", "with", "the", "attribute", "model", "\u2019s", "logits", "during", "during", "training", "."], ["This", "trains", "the", "abstractive", "multi", "document", "summarization", "model", "to", "output", "text", "conditioned", "on", "that", "particular", "attribute", "."], ["The", "same", "classifier", "models", "and", "baseline", "MDS", "models", "were", "used", "to", "evaluate", "this", "approach", "as", "in", "the", "previous", "method", "."], ["We", "show", "that", "this", "approach", "can", "be", "applied", "to", "any", "baseline", "MDS", "architecture", "in", "the", "follow", "up", "ablations", "."]]}, {"text": "Classification Models We first train an XLNet (?) attribute classification model on two different datasets AllTheNews (?) and the MPQA Opinion Corpus dataset (?) in order to determine the sentiment and polarity scores of input text. AllTheNews consists of a body of In order to create a dataset most similar to the input sequences that would be passed through the future discriminator approach as well as the conditionally MDS models, the input text in the dataset was augmented to include all prefix length subsequences with the same label. This corresponds to the prefix length subsequences that will be evaluated for the above approaches. AllTheNews dataset consists of 2.7 million articles from 26 different publications ranging from January 2016 to April 2020 in English. (?). This dataset was augmented with polarity labels according to the news source label. The MPQA Opinion Corpus was chosen over other sentiment analysis datasets as it consists of articles pulled from news articles on a broad range of news sources and is consistent with the approach used in (?)\nMultiNews (?) consists of a varied set of news articles spanning over 1500 sites and their corresponding human written summaries. The average length of the source documents is 2100 tokens which allows the Bart+Longformer model trained with input length 4096 tokens to consume multiple concatenated articles at a time. For all models, each of the input documents are concatenated together and fed into into the model in batches of maximum token size.\nTraining Configurations ACM was trained using with 8 transformer encoder heads and 6 graph decoding layers. Beam size was set to 5 with length penalty factor 0.6 trained with gradient accumulation every 4 steps. Additionally we ran a hyper parameter search to determine the ideal weighting terms for the attribute conditioning module at each phase of summarization. For the ablation studies, BART and BART+Longformer self attention models were trained using the same set of hyperparameters used to train the baseline models. All models were trained using 2 NVIDIA K-90 GPUs. Beam search for each approach set a length penalty of 2.0, max length of 200 tokens, with 4 beams. A BART architecture with 8 transformer heads and 6 decoder layers was used with\ngradient accumulation every 4 steps. The BART model was trained with max token size of 512 and the Bart+Longformer self attention model was trained with max token size of 4096. GraphSum (?) was trained using the same configurations present in the original paper in order to reproduce results with 8 transformer encoder heads and 6 graph decoding layers. To maintain consistency with ACM, beam size was set to 5 with length penalty factor 0.6 trained with gradient accumulation every 4 steps.", "tokenized": [["Classification", "Models", "We", "first", "train", "an", "XLNet", "(", "?", ")", "attribute", "classification", "model", "on", "two", "different", "datasets", "AllTheNews", "(", "?", ")", "and", "the", "MPQA", "Opinion", "Corpus", "dataset", "(", "?", ")", "in", "order", "to", "determine", "the", "sentiment", "and", "polarity", "scores", "of", "input", "text", "."], ["AllTheNews", "consists", "of", "a", "body", "of", "In", "order", "to", "create", "a", "dataset", "most", "similar", "to", "the", "input", "sequences", "that", "would", "be", "passed", "through", "the", "future", "discriminator", "approach", "as", "well", "as", "the", "conditionally", "MDS", "models", ",", "the", "input", "text", "in", "the", "dataset", "was", "augmented", "to", "include", "all", "prefix", "length", "subsequences", "with", "the", "same", "label", "."], ["This", "corresponds", "to", "the", "prefix", "length", "subsequences", "that", "will", "be", "evaluated", "for", "the", "above", "approaches", "."], ["AllTheNews", "dataset", "consists", "of", "2.7", "million", "articles", "from", "26", "different", "publications", "ranging", "from", "January", "2016", "to", "April", "2020", "in", "English", ".", "(", "?", ")", "."], ["This", "dataset", "was", "augmented", "with", "polarity", "labels", "according", "to", "the", "news", "source", "label", "."], ["The", "MPQA", "Opinion", "Corpus", "was", "chosen", "over", "other", "sentiment", "analysis", "datasets", "as", "it", "consists", "of", "articles", "pulled", "from", "news", "articles", "on", "a", "broad", "range", "of", "news", "sources", "and", "is", "consistent", "with", "the", "approach", "used", "in", "(", "?", ")", "\n", "MultiNews", "(", "?", ")", "consists", "of", "a", "varied", "set", "of", "news", "articles", "spanning", "over", "1500", "sites", "and", "their", "corresponding", "human", "written", "summaries", "."], ["The", "average", "length", "of", "the", "source", "documents", "is", "2100", "tokens", "which", "allows", "the", "Bart+Longformer", "model", "trained", "with", "input", "length", "4096", "tokens", "to", "consume", "multiple", "concatenated", "articles", "at", "a", "time", "."], ["For", "all", "models", ",", "each", "of", "the", "input", "documents", "are", "concatenated", "together", "and", "fed", "into", "into", "the", "model", "in", "batches", "of", "maximum", "token", "size", "."], ["\n", "Training", "Configurations", "ACM", "was", "trained", "using", "with", "8", "transformer", "encoder", "heads", "and", "6", "graph", "decoding", "layers", "."], ["Beam", "size", "was", "set", "to", "5", "with", "length", "penalty", "factor", "0.6", "trained", "with", "gradient", "accumulation", "every", "4", "steps", "."], ["Additionally", "we", "ran", "a", "hyper", "parameter", "search", "to", "determine", "the", "ideal", "weighting", "terms", "for", "the", "attribute", "conditioning", "module", "at", "each", "phase", "of", "summarization", "."], ["For", "the", "ablation", "studies", ",", "BART", "and", "BART+Longformer", "self", "attention", "models", "were", "trained", "using", "the", "same", "set", "of", "hyperparameters", "used", "to", "train", "the", "baseline", "models", "."], ["All", "models", "were", "trained", "using", "2", "NVIDIA", "K-90", "GPUs", "."], ["Beam", "search", "for", "each", "approach", "set", "a", "length", "penalty", "of", "2.0", ",", "max", "length", "of", "200", "tokens", ",", "with", "4", "beams", "."], ["A", "BART", "architecture", "with", "8", "transformer", "heads", "and", "6", "decoder", "layers", "was", "used", "with", "\n", "gradient", "accumulation", "every", "4", "steps", "."], ["The", "BART", "model", "was", "trained", "with", "max", "token", "size", "of", "512", "and", "the", "Bart+Longformer", "self", "attention", "model", "was", "trained", "with", "max", "token", "size", "of", "4096", "."], ["GraphSum", "(", "?", ")", "was", "trained", "using", "the", "same", "configurations", "present", "in", "the", "original", "paper", "in", "order", "to", "reproduce", "results", "with", "8", "transformer", "encoder", "heads", "and", "6", "graph", "decoding", "layers", "."], ["To", "maintain", "consistency", "with", "ACM", ",", "beam", "size", "was", "set", "to", "5", "with", "length", "penalty", "factor", "0.6", "trained", "with", "gradient", "accumulation", "every", "4", "steps", "."]]}, {"text": "We evaluate our model on both ROUGE scores as well as perform human annotations to evaluate output summaries for fluency, informativeness, and consistency. (?). In addition we perform a series of ablation studies for each technique presented in order to assess the contribution of each to the final result.", "tokenized": [["We", "evaluate", "our", "model", "on", "both", "ROUGE", "scores", "as", "well", "as", "perform", "human", "annotations", "to", "evaluate", "output", "summaries", "for", "fluency", ",", "informativeness", ",", "and", "consistency", ".", "(", "?", ")", "."], ["In", "addition", "we", "perform", "a", "series", "of", "ablation", "studies", "for", "each", "technique", "presented", "in", "order", "to", "assess", "the", "contribution", "of", "each", "to", "the", "final", "result", "."]]}, {"text": "We primarily compare ACM against other strong baseline model architectures including BART (?), BART+Longformer attention (?) and GraphSum (?).\nWe evaluate each of these methods on the MultiNews dataset. The summarization quality is measured with standard quality metrics ROUGE-1 (overlap of unigrams), ROUGE-2 (overlap of bigrams) and ROUGE-L (longest common subsequence) between the generated summaries and the gold standard references. ROUGE-L is often used as a measure of accessing fluency. (?). We include an appendix with comparisons between the summaries generated by each model and their corresponding gold standard summaries. The first block of results represents the baseline models\u2019 ROUGE scores followed by the results from ACM. Overall performance shows that the sentiment attribute module and the polarity attribute module perform about on par with polarity performing slightly better. This trend holds across the other approaches as well. GraphSum with sentiment outperformed the BART based models as well confirming our hypothesis that learning the relationships between the input data improves attribute consistency in the final output summary.", "tokenized": [["We", "primarily", "compare", "ACM", "against", "other", "strong", "baseline", "model", "architectures", "including", "BART", "(", "?", ")", ",", "BART+Longformer", "attention", "(", "?", ")", "and", "GraphSum", "(", "?", ")", "."], ["\n", "We", "evaluate", "each", "of", "these", "methods", "on", "the", "MultiNews", "dataset", "."], ["The", "summarization", "quality", "is", "measured", "with", "standard", "quality", "metrics", "ROUGE-1", "(", "overlap", "of", "unigrams", ")", ",", "ROUGE-2", "(", "overlap", "of", "bigrams", ")", "and", "ROUGE-L", "(", "longest", "common", "subsequence", ")", "between", "the", "generated", "summaries", "and", "the", "gold", "standard", "references", "."], ["ROUGE-L", "is", "often", "used", "as", "a", "measure", "of", "accessing", "fluency", "."], ["(", "?", ")", "."], ["We", "include", "an", "appendix", "with", "comparisons", "between", "the", "summaries", "generated", "by", "each", "model", "and", "their", "corresponding", "gold", "standard", "summaries", "."], ["The", "first", "block", "of", "results", "represents", "the", "baseline", "models", "\u2019", "ROUGE", "scores", "followed", "by", "the", "results", "from", "ACM", "."], ["Overall", "performance", "shows", "that", "the", "sentiment", "attribute", "module", "and", "the", "polarity", "attribute", "module", "perform", "about", "on", "par", "with", "polarity", "performing", "slightly", "better", "."], ["This", "trend", "holds", "across", "the", "other", "approaches", "as", "well", "."], ["GraphSum", "with", "sentiment", "outperformed", "the", "BART", "based", "models", "as", "well", "confirming", "our", "hypothesis", "that", "learning", "the", "relationships", "between", "the", "input", "data", "improves", "attribute", "consistency", "in", "the", "final", "output", "summary", "."]]}, {"text": "In addition to the automatic evaluation reported above, we also perform a large scale human evaluation study on Amazon Mechanical Turk on the summary outputs. We randomly select 182 input test summaries from the MultiNews dataset and the corresponding output summaries generated by ACM. In order to assess the quality of the model irrespective of the classifier model chosen, we randomly selected output summaries between the two classifiers. We use the ACM model without conditioning as the baseline model. Annotators assess the overall quality of the summaries based on three different criteria: (1) informativeness, (2) fluency and (3) repetitive content. Informativeness is defined as the number of unique facts / pieces of information present in the summary. Fluency is defined as the readability of the text accounting for good grammar, noun phrases and logical flow of information. Repetitive content comes from repeated words, phrases, or ideas throughout the output summary. Each of these attributes were assessed on a scale of 1 (worst) to 5 (best). According to this scale, a high score is preferable for fluency and informativeness and a lower score is preferable for repetitive content.", "tokenized": [["In", "addition", "to", "the", "automatic", "evaluation", "reported", "above", ",", "we", "also", "perform", "a", "large", "scale", "human", "evaluation", "study", "on", "Amazon", "Mechanical", "Turk", "on", "the", "summary", "outputs", "."], ["We", "randomly", "select", "182", "input", "test", "summaries", "from", "the", "MultiNews", "dataset", "and", "the", "corresponding", "output", "summaries", "generated", "by", "ACM", "."], ["In", "order", "to", "assess", "the", "quality", "of", "the", "model", "irrespective", "of", "the", "classifier", "model", "chosen", ",", "we", "randomly", "selected", "output", "summaries", "between", "the", "two", "classifiers", "."], ["We", "use", "the", "ACM", "model", "without", "conditioning", "as", "the", "baseline", "model", "."], ["Annotators", "assess", "the", "overall", "quality", "of", "the", "summaries", "based", "on", "three", "different", "criteria", ":", "(", "1", ")", "informativeness", ",", "(", "2", ")", "fluency", "and", "(", "3", ")", "repetitive", "content", "."], ["Informativeness", "is", "defined", "as", "the", "number", "of", "unique", "facts", "/", "pieces", "of", "information", "present", "in", "the", "summary", "."], ["Fluency", "is", "defined", "as", "the", "readability", "of", "the", "text", "accounting", "for", "good", "grammar", ",", "noun", "phrases", "and", "logical", "flow", "of", "information", "."], ["Repetitive", "content", "comes", "from", "repeated", "words", ",", "phrases", ",", "or", "ideas", "throughout", "the", "output", "summary", "."], ["Each", "of", "these", "attributes", "were", "assessed", "on", "a", "scale", "of", "1", "(", "worst", ")", "to", "5", "(", "best", ")", "."], ["According", "to", "this", "scale", ",", "a", "high", "score", "is", "preferable", "for", "fluency", "and", "informativeness", "and", "a", "lower", "score", "is", "preferable", "for", "repetitive", "content", "."]]}, {"text": "In order to determine the contribution of each method used within the ACM model, we performed additional ablations and model analysis. The key ablation studies included evaluating each approach on one of the baseline models, BART and BART + Longformer. We analyzed the results here both in terms of achieving higher ROUGE scores as well as maintaining information consistency. Table 5 presents the ROUGE scores for each of the approaches. We note that there are improvements from each approach individually with respect to the ROUGE score with attribute future discriminators performing marginally better than the other approaches. BART+Longformer achieves overall better performance on ROUGE scores as compared to BART primarily due to the longer input sequence lengths passed in. In addition to evaluating on ROUGE, an analysis done on how well each approach was able to preserve the overall attribute conditioning.\nAdditionally we conducted a qualitative analysis of how well each approach was able to condition for polarity and sentiment in the output summaries by evaluating the summaries through the trained attribute conditioning module as shown in Table 5. This shows strong out of domain analysis results as the attribute conditioning module for polarity was trained on a different dataset, AllTheNews, and evaluated on the MultiNews dataset for MDS. This ablation study shows that This analysis showed that MDS with future discriminators is the strongest attribute conditioning model. Since the sentiment and polarity of articles as determined by the XlNet classifiers are used to compute the opinion of an article, we used these models to analyze the MultiNews dataset.", "tokenized": [["In", "order", "to", "determine", "the", "contribution", "of", "each", "method", "used", "within", "the", "ACM", "model", ",", "we", "performed", "additional", "ablations", "and", "model", "analysis", "."], ["The", "key", "ablation", "studies", "included", "evaluating", "each", "approach", "on", "one", "of", "the", "baseline", "models", ",", "BART", "and", "BART", "+", "Longformer", "."], ["We", "analyzed", "the", "results", "here", "both", "in", "terms", "of", "achieving", "higher", "ROUGE", "scores", "as", "well", "as", "maintaining", "information", "consistency", "."], ["Table", "5", "presents", "the", "ROUGE", "scores", "for", "each", "of", "the", "approaches", "."], ["We", "note", "that", "there", "are", "improvements", "from", "each", "approach", "individually", "with", "respect", "to", "the", "ROUGE", "score", "with", "attribute", "future", "discriminators", "performing", "marginally", "better", "than", "the", "other", "approaches", "."], ["BART+Longformer", "achieves", "overall", "better", "performance", "on", "ROUGE", "scores", "as", "compared", "to", "BART", "primarily", "due", "to", "the", "longer", "input", "sequence", "lengths", "passed", "in", "."], ["In", "addition", "to", "evaluating", "on", "ROUGE", ",", "an", "analysis", "done", "on", "how", "well", "each", "approach", "was", "able", "to", "preserve", "the", "overall", "attribute", "conditioning", "."], ["\n", "Additionally", "we", "conducted", "a", "qualitative", "analysis", "of", "how", "well", "each", "approach", "was", "able", "to", "condition", "for", "polarity", "and", "sentiment", "in", "the", "output", "summaries", "by", "evaluating", "the", "summaries", "through", "the", "trained", "attribute", "conditioning", "module", "as", "shown", "in", "Table", "5", "."], ["This", "shows", "strong", "out", "of", "domain", "analysis", "results", "as", "the", "attribute", "conditioning", "module", "for", "polarity", "was", "trained", "on", "a", "different", "dataset", ",", "AllTheNews", ",", "and", "evaluated", "on", "the", "MultiNews", "dataset", "for", "MDS", "."], ["This", "ablation", "study", "shows", "that", "This", "analysis", "showed", "that", "MDS", "with", "future", "discriminators", "is", "the", "strongest", "attribute", "conditioning", "model", "."], ["Since", "the", "sentiment", "and", "polarity", "of", "articles", "as", "determined", "by", "the", "XlNet", "classifiers", "are", "used", "to", "compute", "the", "opinion", "of", "an", "article", ",", "we", "used", "these", "models", "to", "analyze", "the", "MultiNews", "dataset", "."]]}, {"text": "In this work, we present a novel approach ACM, attribute conditioned multi document summarization, that sets the new state of the art for multi document summarization. It tackles the challenge of\naddressing conflicting information in multi document summarization by conditioning for a desired attribute and preserving consistency in the final output summary. Through the attribute future discriminators we are able to compose different conditional attribute models with a pretrained MDS model during evaluation. To our knowledge, this is the first approach taken to effectively decouple conflicting information by conditioning for a certain attribute in the output summary. This approach shows strong gains in ROGUE score over baseline multi document summarization approaches and shows gains in fluency and informativeness as well as a reduction in repetitiveness as shown through a human annotation analysis study.", "tokenized": [["In", "this", "work", ",", "we", "present", "a", "novel", "approach", "ACM", ",", "attribute", "conditioned", "multi", "document", "summarization", ",", "that", "sets", "the", "new", "state", "of", "the", "art", "for", "multi", "document", "summarization", "."], ["It", "tackles", "the", "challenge", "of", "\n", "addressing", "conflicting", "information", "in", "multi", "document", "summarization", "by", "conditioning", "for", "a", "desired", "attribute", "and", "preserving", "consistency", "in", "the", "final", "output", "summary", "."], ["Through", "the", "attribute", "future", "discriminators", "we", "are", "able", "to", "compose", "different", "conditional", "attribute", "models", "with", "a", "pretrained", "MDS", "model", "during", "evaluation", "."], ["To", "our", "knowledge", ",", "this", "is", "the", "first", "approach", "taken", "to", "effectively", "decouple", "conflicting", "information", "by", "conditioning", "for", "a", "certain", "attribute", "in", "the", "output", "summary", "."], ["This", "approach", "shows", "strong", "gains", "in", "ROGUE", "score", "over", "baseline", "multi", "document", "summarization", "approaches", "and", "shows", "gains", "in", "fluency", "and", "informativeness", "as", "well", "as", "a", "reduction", "in", "repetitiveness", "as", "shown", "through", "a", "human", "annotation", "analysis", "study", "."]]}, {"text": "Selected prediction samples for baseline model against each experiment.\nA.1 EXAMPLE 1\nBART Baseline \u2013 The Siskiyou County Board of Supervisors voted 4-1 Tuesday to support the state\u2019s split from California, the Sacramento Times-Standard reports. The move would bring representation to rural North State counties that are beholden to the whims of representatives of the more heavily populated Southern California and free them from burdensome state regulations, the Times-Standard reports. \u201dWe have to have government that\u2019s local, understands our issues and has empathy,\u201d says a Scott Valley rancher who is leading the charge to form a new state.\nACM conditioned with Polarity Not a big fan of Southern California? Neither is Northern California, apparently. Supervisors in rural Siskiyou County voted 4-1 Tuesday in favor of seceding from the state, reports the TimesStandard. The county thinks state officials in Sacramento are too focused on the big metro areas of the south and thus want to form a state of its own\u2014to be called Jefferson. Supporters want to invite other northern counties, and perhaps a few from Oregon, to join, too, notes the Los Angeles Times. \u201dWe have to have government that\u2019s local, understands our issues, and has empathy,\u201d says one local rancher. It\u2019s clearly a very long-shot bid, one that would eventually require the blessing of both the state legislature and the US Congress. Still, neighboring Humboldt County is expected to meet soon to consider the idea. (Meanwhile, some counties in Colorado want to form a new state of their own.)\nA.2 EXAMPLE 2\nBART Baseline \u2013 Microsoft\u2019s acquisition of Nokia is aimed at building a devices and services strategy, but the joint company won\u2019t take the same form as Apple, reports ZDNet. Microsoft has been working on its evolution into a devices and services company, away from the services business it traditionally was, for several years now with limited success. Its acquisition of most of Nokia is the latest acceleration of that strategy\u2014to move further away from the moribund world of the beige desktop and towards the sunlit world of smartphones and tablets. Owning the desktop (via Windows) and building additional services on top.\nACM conditioned with polarity Why did Microsoft buy Nokia\u2019s phone business? We now know Microsoft\u2019s answer: The computing giant released a 30-slide presentation today arguing that the move will improve Microsoft\u2019s margins on Windows phones, which will allow it to invest more in the platform, which will accelerate sales and market share growth, the Washington Post reports. But John Herrman at Buzzfeed has another explanation: \u201dFear of dying alone.\u201d Here\u2019s what he and other pundits are saying: The presentation \u201dmanages to sound both insane and uninspiring, outlining modest goals that still sound unrealistic,\u201d Herman argues\u2014like capturing a whole 15% of the smartphone market. \u201dIt\u2019s a fitting end for the close of Microsoft\u2019s Ballmer era, during which the company . . . missed out on the most important change in consumer electronics in decades\u201d while remaining profitable in unglamorous ways.\nA.3 EXAMPLE 3\nBART + Longformer Baseline \u2013 The Supreme Court has a new term that could be a blow to the labor unions and roll back affirmative action at state universities, reports POLITICO. The justices are already facing a batch of petitions involving the rights of religious institutions to opt out of providing contraception under Obamacare. The case, which was brought by California schoolteacher Rebecca Friedrichs and other teachers, was brought by Orange County, Calif., schoolteacher Rebecca Friedrichs and other teachers, who are arguing that forcing government employees to pay union dues violates their First Amendment rights.\nACM conditioned with polarity The Supreme Court is facing a docket of high-profile political cases that will test whether recent liberal victories were more fluke or firm conviction, the New York Times reports. The court\u2014which is divided 5-4 for conservatives, but saw Justice Roberts vote liberal on Obamacare and same-sex marriage\u2014will look at cases including unions, affirmative action, and possibly abortion. A primer: Unions: Since 1977, unions have been allowed to charge non-union workers for dues that go to collective bargaining efforts, but not political ones. Now California teachers have brought a case saying collective bargaining is itself political. \u201dIt could set the stage for a Citizens United-style reconsideration in the area of union dues,\u201d a lawyer says. Affirmative Action: Abigail Fisher says that being white played a role in the University of Texas denying her admission back in 2008.\nA.4 EXAMPLE 4\nBART + Longformer Baseline \u2013 A preservationist says he has found evidence that a Manhattan building is the former site of an 18th-century tavern where George Washington is believed to have enjoyed a celebratory drink during the American Revolution. If it is indeed the home of the legendary watering hole, the discovery could mean that the building that is perhaps Manhattan\u2019s oldest is slated to demolished. \u201dAfter the English had marched up the Bowery and out of the city (in 1783), George Washington and Governor (George) Clinton stopped at the Bull\u2019s Head (tavern).\nACM conditioned with sentiment - In 1783, after the British soldiers left New York City, George Washington is believed to have stopped for a celebratory drink at the Bull\u2019s Head tavern. Now a preservationist thinks he\u2019s found the historic site\u2014and if he\u2019s right, it could be the oldest building in Manhattan. Adam Woodward had heard that the building at 50 Bowery, currently scheduled to be demolished so a hotel can go up, might have \u201dthe Bull\u2019s Head\u2019s structure, cellar, bones,\u201d he tells CBS New York. So he searched the basement, and \u201dfound myself in what I am pretty certain is the 1750s historic tavern,\u201d he says. Specifically, he found what he thinks are hand-hewn and hand-planed joists and foundation walls from the Colonial era. Since that time, the building has housed a drugstore, a Chinese restaurant, and a beer garden, among other things.", "tokenized": [["Selected", "prediction", "samples", "for", "baseline", "model", "against", "each", "experiment", "."], ["\n", "A.1", "EXAMPLE", "1", "\n", "BART", "Baseline", "\u2013", "The", "Siskiyou", "County", "Board", "of", "Supervisors", "voted", "4", "-", "1", "Tuesday", "to", "support", "the", "state", "\u2019s", "split", "from", "California", ",", "the", "Sacramento", "Times-Standard", "reports", "."], ["The", "move", "would", "bring", "representation", "to", "rural", "North", "State", "counties", "that", "are", "beholden", "to", "the", "whims", "of", "representatives", "of", "the", "more", "heavily", "populated", "Southern", "California", "and", "free", "them", "from", "burdensome", "state", "regulations", ",", "the", "Times-Standard", "reports", ".", "\u201d", "We", "have", "to", "have", "government", "that", "\u2019s", "local", ",", "understands", "our", "issues", "and", "has", "empathy", ",", "\u201d", "says", "a", "Scott", "Valley", "rancher", "who", "is", "leading", "the", "charge", "to", "form", "a", "new", "state", "."], ["\n", "ACM", "conditioned", "with", "Polarity", "Not", "a", "big", "fan", "of", "Southern", "California", "?", "Neither", "is", "Northern", "California", ",", "apparently", "."], ["Supervisors", "in", "rural", "Siskiyou", "County", "voted", "4", "-", "1", "Tuesday", "in", "favor", "of", "seceding", "from", "the", "state", ",", "reports", "the", "TimesStandard", "."], ["The", "county", "thinks", "state", "officials", "in", "Sacramento", "are", "too", "focused", "on", "the", "big", "metro", "areas", "of", "the", "south", "and", "thus", "want", "to", "form", "a", "state", "of", "its", "own", "\u2014", "to", "be", "called", "Jefferson", ".", "Supporters", "want", "to", "invite", "other", "northern", "counties", ",", "and", "perhaps", "a", "few", "from", "Oregon", ",", "to", "join", ",", "too", ",", "notes", "the", "Los", "Angeles", "Times", ".", "\u201d", "We", "have", "to", "have", "government", "that", "\u2019s", "local", ",", "understands", "our", "issues", ",", "and", "has", "empathy", ",", "\u201d", "says", "one", "local", "rancher", "."], ["It", "\u2019s", "clearly", "a", "very", "long-shot", "bid", ",", "one", "that", "would", "eventually", "require", "the", "blessing", "of", "both", "the", "state", "legislature", "and", "the", "US", "Congress", "."], ["Still", ",", "neighboring", "Humboldt", "County", "is", "expected", "to", "meet", "soon", "to", "consider", "the", "idea", "."], ["(", "Meanwhile", ",", "some", "counties", "in", "Colorado", "want", "to", "form", "a", "new", "state", "of", "their", "own", ".", ")", "\n", "A.2", "EXAMPLE", "2", "\n", "BART", "Baseline", "\u2013", "Microsoft", "\u2019s", "acquisition", "of", "Nokia", "is", "aimed", "at", "building", "a", "devices", "and", "services", "strategy", ",", "but", "the", "joint", "company", "wo", "n\u2019t", "take", "the", "same", "form", "as", "Apple", ",", "reports", "ZDNet", "."], ["Microsoft", "has", "been", "working", "on", "its", "evolution", "into", "a", "devices", "and", "services", "company", ",", "away", "from", "the", "services", "business", "it", "traditionally", "was", ",", "for", "several", "years", "now", "with", "limited", "success", "."], ["Its", "acquisition", "of", "most", "of", "Nokia", "is", "the", "latest", "acceleration", "of", "that", "strategy", "\u2014", "to", "move", "further", "away", "from", "the", "moribund", "world", "of", "the", "beige", "desktop", "and", "towards", "the", "sunlit", "world", "of", "smartphones", "and", "tablets", "."], ["Owning", "the", "desktop", "(", "via", "Windows", ")", "and", "building", "additional", "services", "on", "top", "."], ["\n", "ACM", "conditioned", "with", "polarity", "Why", "did", "Microsoft", "buy", "Nokia", "\u2019s", "phone", "business", "?", "We", "now", "know", "Microsoft", "\u2019s", "answer", ":", "The", "computing", "giant", "released", "a", "30-slide", "presentation", "today", "arguing", "that", "the", "move", "will", "improve", "Microsoft", "\u2019s", "margins", "on", "Windows", "phones", ",", "which", "will", "allow", "it", "to", "invest", "more", "in", "the", "platform", ",", "which", "will", "accelerate", "sales", "and", "market", "share", "growth", ",", "the", "Washington", "Post", "reports", "."], ["But", "John", "Herrman", "at", "Buzzfeed", "has", "another", "explanation", ":", "\u201d", "Fear", "of", "dying", "alone", ".", "\u201d", "Here", "\u2019s", "what", "he", "and", "other", "pundits", "are", "saying", ":", "The", "presentation", "\u201d", "manages", "to", "sound", "both", "insane", "and", "uninspiring", ",", "outlining", "modest", "goals", "that", "still", "sound", "unrealistic", ",", "\u201d", "Herman", "argues", "\u2014", "like", "capturing", "a", "whole", "15", "%", "of", "the", "smartphone", "market", ".", "\u201d", "It", "\u2019s", "a", "fitting", "end", "for", "the", "close", "of", "Microsoft", "\u2019s", "Ballmer", "era", ",", "during", "which", "the", "company", ".", ".", ".", "missed", "out", "on", "the", "most", "important", "change", "in", "consumer", "electronics", "in", "decades", "\u201d", "while", "remaining", "profitable", "in", "unglamorous", "ways", "."], ["\n", "A.3", "EXAMPLE", "3", "\n", "BART", "+", "Longformer", "Baseline", "\u2013", "The", "Supreme", "Court", "has", "a", "new", "term", "that", "could", "be", "a", "blow", "to", "the", "labor", "unions", "and", "roll", "back", "affirmative", "action", "at", "state", "universities", ",", "reports", "POLITICO", "."], ["The", "justices", "are", "already", "facing", "a", "batch", "of", "petitions", "involving", "the", "rights", "of", "religious", "institutions", "to", "opt", "out", "of", "providing", "contraception", "under", "Obamacare", "."], ["The", "case", ",", "which", "was", "brought", "by", "California", "schoolteacher", "Rebecca", "Friedrichs", "and", "other", "teachers", ",", "was", "brought", "by", "Orange", "County", ",", "Calif.", ",", "schoolteacher", "Rebecca", "Friedrichs", "and", "other", "teachers", ",", "who", "are", "arguing", "that", "forcing", "government", "employees", "to", "pay", "union", "dues", "violates", "their", "First", "Amendment", "rights", "."], ["\n", "ACM", "conditioned", "with", "polarity", "The", "Supreme", "Court", "is", "facing", "a", "docket", "of", "high-profile", "political", "cases", "that", "will", "test", "whether", "recent", "liberal", "victories", "were", "more", "fluke", "or", "firm", "conviction", ",", "the", "New", "York", "Times", "reports", "."], ["The", "court", "\u2014", "which", "is", "divided", "5", "-", "4", "for", "conservatives", ",", "but", "saw", "Justice", "Roberts", "vote", "liberal", "on", "Obamacare", "and", "same-sex", "marriage", "\u2014", "will", "look", "at", "cases", "including", "unions", ",", "affirmative", "action", ",", "and", "possibly", "abortion", "."], ["A", "primer", ":", "Unions", ":", "Since", "1977", ",", "unions", "have", "been", "allowed", "to", "charge", "non-union", "workers", "for", "dues", "that", "go", "to", "collective", "bargaining", "efforts", ",", "but", "not", "political", "ones", "."], ["Now", "California", "teachers", "have", "brought", "a", "case", "saying", "collective", "bargaining", "is", "itself", "political", ".", "\u201d", "It", "could", "set", "the", "stage", "for", "a", "Citizens", "United-style", "reconsideration", "in", "the", "area", "of", "union", "dues", ",", "\u201d", "a", "lawyer", "says", "."], ["Affirmative", "Action", ":", "Abigail", "Fisher", "says", "that", "being", "white", "played", "a", "role", "in", "the", "University", "of", "Texas", "denying", "her", "admission", "back", "in", "2008", "."], ["\n", "A.4", "EXAMPLE", "4", "\n", "BART", "+", "Longformer", "Baseline", "\u2013", "A", "preservationist", "says", "he", "has", "found", "evidence", "that", "a", "Manhattan", "building", "is", "the", "former", "site", "of", "an", "18th-century", "tavern", "where", "George", "Washington", "is", "believed", "to", "have", "enjoyed", "a", "celebratory", "drink", "during", "the", "American", "Revolution", ".", "If", "it", "is", "indeed", "the", "home", "of", "the", "legendary", "watering", "hole", ",", "the", "discovery", "could", "mean", "that", "the", "building", "that", "is", "perhaps", "Manhattan", "\u2019s", "oldest", "is", "slated", "to", "demolished", ".", "\u201d", "After", "the", "English", "had", "marched", "up", "the", "Bowery", "and", "out", "of", "the", "city", "(", "in", "1783", ")", ",", "George", "Washington", "and", "Governor", "(", "George", ")", "Clinton", "stopped", "at", "the", "Bull", "\u2019s", "Head", "(", "tavern", ")", "."], ["\n", "ACM", "conditioned", "with", "sentiment", "-", "In", "1783", ",", "after", "the", "British", "soldiers", "left", "New", "York", "City", ",", "George", "Washington", "is", "believed", "to", "have", "stopped", "for", "a", "celebratory", "drink", "at", "the", "Bull", "\u2019s", "Head", "tavern", "."], ["Now", "a", "preservationist", "thinks", "he", "\u2019s", "found", "the", "historic", "site", "\u2014", "and", "if", "he", "\u2019s", "right", ",", "it", "could", "be", "the", "oldest", "building", "in", "Manhattan", "."], ["Adam", "Woodward", "had", "heard", "that", "the", "building", "at", "50", "Bowery", ",", "currently", "scheduled", "to", "be", "demolished", "so", "a", "hotel", "can", "go", "up", ",", "might", "have", "\u201d", "the", "Bull", "\u2019s", "Head", "\u2019s", "structure", ",", "cellar", ",", "bones", ",", "\u201d", "he", "tells", "CBS", "New", "York", "."], ["So", "he", "searched", "the", "basement", ",", "and", "\u201d", "found", "myself", "in", "what", "I", "am", "pretty", "certain", "is", "the", "1750s", "historic", "tavern", ",", "\u201d", "he", "says", ".", "Specifically", ",", "he", "found", "what", "he", "thinks", "are", "hand-hewn", "and", "hand-planed", "joists", "and", "foundation", "walls", "from", "the", "Colonial", "era", "."], ["Since", "that", "time", ",", "the", "building", "has", "housed", "a", "drugstore", ",", "a", "Chinese", "restaurant", ",", "and", "a", "beer", "garden", ",", "among", "other", "things", "."]]}], "summaries": ["This paper proposes a transformer-based abstractive multi-document summarization approach conditioned on attribute classification models, which check for sentiment and polarity attributes to ensure consistency of the generated summaries across multiple documents. The attribute conditioning classification models are built in a modular fashion to guide summary viewpoint consistency throughout the various stages of the summarization process and trained on different datasets than the dataset for which the downstream summarization task is evaluated on. Experiments and ablation study show the effectiveness of the decomposable conditioning component compared to several baselines without the conditioning component via using both automatic evaluation (ROUGE) and human evaluation.", "To  make the generated summary be little affected by the conflicting information among documents, this paper introduces an attribute conditioned module for the task of abstractive multi-document summarization, which aims to generate a summary of the given multiple documents. The proposed module first applies an external classifier, i.e., XLNet, to predict the attribute of the input, and then improves the state-of-the-art model, i.e., GraphSum, for both the encoder and the decoder. For the encoder, the attribute score of each paragraph is computed and then used to measure the consistency among paragraphs for better learning the graph-based representation of the input. For the decoder, the attribute is predicted based on the partial predicted sequence generated so far for better generating the next token. The module is evaluated on both automatic and human metrics, and the experimental results show the effectiveness of the proposed module.", "This paper presents a method for multi-document summarization that considers contradictory information present in the source documents. The presented method is an extension of GraphSum with attribute conditioning modules in the encoder and decoder. Attribute discriminators are incorporated into the model so that it can predict attributes when generating a summary. This study reports ROUGE scores, human evaluation, and ablation study on MultiNews dataset.", "This paper proposes an attribute (sentiment or polarity) conditioned multi-document summarization framework to address of issue of conflicting information in input documents. Attribute classifiers are trained in the pre-stage and then are used to get the attribute information of the content. Experimental results show that the proposed framework can obtain better ROUGE than the baseline BART."]}